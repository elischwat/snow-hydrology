{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CEWA 568 Snow Hydrology - Spring 2023\n",
    "## Lab 2-2: Downloading and plotting data snow pillow, temperature, and precipitation data from Kettle Ponds.\n",
    "\n",
    "Written by Eli Schwat (elilouis@uw.edu) - March 2023\n",
    "\n",
    "---\n",
    "\n",
    "This lab will introduce you to downloading and plotting data from the Sublimation of Snow (SoS) field campaign. The SoS field site is at Kettle Ponds within the East River valley near Crested Butte, CO. \n",
    "\n",
    "First we will download data from an FTP server hosted by the Earth Observing Laboratory at the University Corporation for Atmospheric Research, who assisted with the logistics of the SoS field campaign. We open up the downloaded SoS NetCDF files with the Xarray library.\n",
    "\n",
    "Then, we will read in a data file (stored locally) with precipitation guage data from an instrument maintained by the NOAA SPLASH campaign. This dataset comes in whitespace-delimited text files (like a csv but with spaces instead of commas). We open up this data with the Pandas library.\n",
    "\n",
    "Finally, we plot both datasets using Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries we'll need\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-provided inputs\n",
    "\n",
    "Here we define a few variables to be used in the notebook. You may or may not want to change these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to a directory you want to download the SoS files to\n",
    "download_directory = \"../data/sosnoqc\"\n",
    "\n",
    "# Provide the paths to the precipitation file. This file was provided to you, you can either\n",
    "# specify the path below or put the file in the same directory as this notebook.\n",
    "precipitation_file = 'precipitation.nc'\n",
    "\n",
    "# Provide a the start and end dates you want to download\n",
    "start_date = '20221201'\n",
    "end_date = '20230327'\n",
    "\n",
    "# Provide the specific variables you want to extract from the dataset.\n",
    "# We provide these in advance because the datasets are very large and, \n",
    "# when we open them, we want to immediately extract only the data we want.\n",
    "VARIABLES = [\n",
    "    'SWE_p1_c',\n",
    "    'SWE_p2_c',\n",
    "    'SWE_p3_c',\n",
    "    'SWE_p4_c',\n",
    "    'T_2m_c',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and open SoS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download SoS NetCDF files from an NCAR FTP server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for downloading daily SoS NetCDF filesm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sos_data_day(date, local_download_dir, cache=True):\n",
    "    \"\"\"Download a netcdf file from the ftp url provided by the Earth Observing \n",
    "    Laboratory at NCAR. Data comes in one NetCDF file per day, Reynolds-Averaged \n",
    "    with a 5-minute averaging period.\n",
    "\n",
    "    Args:\n",
    "        date (str, optional): String representing a date in format '%Y%m%d'. Defaults to '20221101'.\n",
    "        local_download_dir (str, optional): Directory to which files will be downloaded. Defaults \n",
    "                    to 'sosnoqc'; this directory will be created if it  does not already exist.\n",
    "        cache (bool, optional): If True, looks in `local_download_dir` for requested  file, if it \n",
    "                    exists, does not redownload file. If False, downloads file from the FTP server.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Relative path to the downloaded file\n",
    "    \"\"\"\n",
    "    base_url = 'ftp.eol.ucar.edu'\n",
    "    path = 'pub/archive/isfs/projects/SOS/netcdf/noqc_geo'\n",
    "    file = f'isfs_{date}.nc'\n",
    "    os.makedirs(local_download_dir, exist_ok=True)\n",
    "    full_file_path = os.path.join('ftp://', base_url, path, file)\n",
    "    download_file_path = os.path.join(local_download_dir, file)\n",
    "    if cache and os.path.isfile(download_file_path):\n",
    "        print(f\"Caching...skipping download for {date}\")\n",
    "    else:\n",
    "        urllib.request.urlretrieve(\n",
    "            full_file_path,\n",
    "            download_file_path   \n",
    "        )\n",
    "    return download_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function defined above to download all the files we specified. When you run the cell below this comment, this will ~3-5 minutes if you are downloading more than a few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching...skipping download for 20221201\n",
      "Caching...skipping download for 20221202\n",
      "Caching...skipping download for 20221203\n",
      "Caching...skipping download for 20221204\n",
      "Caching...skipping download for 20221205\n",
      "Caching...skipping download for 20221206\n",
      "Caching...skipping download for 20221207\n",
      "Caching...skipping download for 20221208\n",
      "Caching...skipping download for 20221209\n",
      "Caching...skipping download for 20221210\n",
      "Caching...skipping download for 20221211\n",
      "Caching...skipping download for 20221212\n",
      "Caching...skipping download for 20221213\n",
      "Caching...skipping download for 20221214\n",
      "Caching...skipping download for 20221215\n",
      "Caching...skipping download for 20221216\n",
      "Caching...skipping download for 20221217\n",
      "Caching...skipping download for 20221218\n",
      "Caching...skipping download for 20221219\n",
      "Caching...skipping download for 20221220\n",
      "Caching...skipping download for 20221221\n",
      "Caching...skipping download for 20221222\n",
      "Caching...skipping download for 20221223\n",
      "Caching...skipping download for 20221224\n",
      "Caching...skipping download for 20221225\n",
      "Caching...skipping download for 20221226\n",
      "Caching...skipping download for 20221227\n",
      "Caching...skipping download for 20221228\n",
      "Caching...skipping download for 20221229\n",
      "Caching...skipping download for 20221230\n",
      "Caching...skipping download for 20221231\n",
      "Caching...skipping download for 20230101\n",
      "Caching...skipping download for 20230102\n",
      "Caching...skipping download for 20230103\n",
      "Caching...skipping download for 20230104\n",
      "Caching...skipping download for 20230105\n",
      "Caching...skipping download for 20230106\n",
      "Caching...skipping download for 20230107\n",
      "Caching...skipping download for 20230108\n",
      "Caching...skipping download for 20230109\n",
      "Caching...skipping download for 20230110\n",
      "Caching...skipping download for 20230111\n",
      "Caching...skipping download for 20230112\n",
      "Caching...skipping download for 20230113\n",
      "Caching...skipping download for 20230114\n",
      "Caching...skipping download for 20230115\n",
      "Caching...skipping download for 20230116\n",
      "Caching...skipping download for 20230117\n",
      "Caching...skipping download for 20230118\n",
      "Caching...skipping download for 20230119\n",
      "Caching...skipping download for 20230120\n",
      "Caching...skipping download for 20230121\n",
      "Caching...skipping download for 20230122\n",
      "Caching...skipping download for 20230123\n",
      "Caching...skipping download for 20230124\n",
      "Caching...skipping download for 20230125\n",
      "Caching...skipping download for 20230126\n",
      "Caching...skipping download for 20230127\n",
      "Caching...skipping download for 20230128\n",
      "Caching...skipping download for 20230129\n",
      "Caching...skipping download for 20230130\n",
      "Caching...skipping download for 20230131\n",
      "Caching...skipping download for 20230201\n",
      "Caching...skipping download for 20230202\n",
      "Caching...skipping download for 20230203\n",
      "Caching...skipping download for 20230204\n",
      "Caching...skipping download for 20230205\n",
      "Caching...skipping download for 20230206\n",
      "Caching...skipping download for 20230207\n",
      "Caching...skipping download for 20230208\n",
      "Caching...skipping download for 20230209\n",
      "Caching...skipping download for 20230210\n",
      "Caching...skipping download for 20230211\n",
      "Caching...skipping download for 20230212\n",
      "Caching...skipping download for 20230213\n",
      "Caching...skipping download for 20230214\n",
      "Caching...skipping download for 20230215\n",
      "Caching...skipping download for 20230216\n",
      "Caching...skipping download for 20230217\n",
      "Caching...skipping download for 20230218\n",
      "Caching...skipping download for 20230219\n",
      "Caching...skipping download for 20230220\n",
      "Caching...skipping download for 20230221\n",
      "Caching...skipping download for 20230222\n",
      "Caching...skipping download for 20230223\n",
      "Caching...skipping download for 20230224\n",
      "Caching...skipping download for 20230225\n",
      "Caching...skipping download for 20230226\n",
      "Caching...skipping download for 20230227\n",
      "Caching...skipping download for 20230228\n",
      "Caching...skipping download for 20230301\n",
      "Caching...skipping download for 20230302\n",
      "Caching...skipping download for 20230303\n",
      "Caching...skipping download for 20230304\n",
      "Caching...skipping download for 20230305\n",
      "Caching...skipping download for 20230306\n",
      "Caching...skipping download for 20230307\n",
      "Caching...skipping download for 20230308\n",
      "Caching...skipping download for 20230309\n",
      "Caching...skipping download for 20230310\n",
      "Caching...skipping download for 20230311\n",
      "Caching...skipping download for 20230312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m datelist \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mdate_range(\n\u001b[1;32m      3\u001b[0m     dt\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(start_date, \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m     dt\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(end_date, \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      6\u001b[0m )\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m \u001b[39m# Download one daily file for each date in the datelist. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# cache = True so that when we rerun the notebook in the future, we don't have to redownload the data.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m files \u001b[39m=\u001b[39m [download_sos_data_day(date, download_directory, cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mfor\u001b[39;49;00m date \u001b[39min\u001b[39;49;00m datelist]\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m datelist \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mdate_range(\n\u001b[1;32m      3\u001b[0m     dt\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(start_date, \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m     dt\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mstrptime(end_date, \u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      6\u001b[0m )\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      8\u001b[0m \u001b[39m# Download one daily file for each date in the datelist. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# cache = True so that when we rerun the notebook in the future, we don't have to redownload the data.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m files \u001b[39m=\u001b[39m [download_sos_data_day(date, download_directory, cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m date \u001b[39min\u001b[39;00m datelist]\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mdownload_sos_data_day\u001b[0;34m(date, local_download_dir, cache)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCaching...skipping download for \u001b[39m\u001b[39m{\u001b[39;00mdate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlretrieve(\n\u001b[1;32m     26\u001b[0m         full_file_path,\n\u001b[1;32m     27\u001b[0m         download_file_path   \n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m download_file_path\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[39m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m contextlib\u001b[39m.\u001b[39mclosing(urlopen(url, data)) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39minfo()\n\u001b[1;32m    244\u001b[0m     \u001b[39m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:1572\u001b[0m, in \u001b[0;36mFTPHandler.ftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[39mif\u001b[39;00m attr\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m   1570\u001b[0m        value \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1571\u001b[0m         \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mupper()\n\u001b[0;32m-> 1572\u001b[0m fp, retrlen \u001b[39m=\u001b[39m fw\u001b[39m.\u001b[39;49mretrfile(file, \u001b[39mtype\u001b[39;49m)\n\u001b[1;32m   1573\u001b[0m headers \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1574\u001b[0m mtype \u001b[39m=\u001b[39m mimetypes\u001b[39m.\u001b[39mguess_type(req\u001b[39m.\u001b[39mfull_url)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/urllib/request.py:2442\u001b[0m, in \u001b[0;36mftpwrapper.retrfile\u001b[0;34m(self, file, type)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2441\u001b[0m     cmd \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRETR \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m file\n\u001b[0;32m-> 2442\u001b[0m     conn, retrlen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mftp\u001b[39m.\u001b[39;49mntransfercmd(cmd)\n\u001b[1;32m   2443\u001b[0m \u001b[39mexcept\u001b[39;00m ftplib\u001b[39m.\u001b[39merror_perm \u001b[39mas\u001b[39;00m reason:\n\u001b[1;32m   2444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(reason)[:\u001b[39m3\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m550\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/ftplib.py:359\u001b[0m, in \u001b[0;36mFTP.ntransfercmd\u001b[0;34m(self, cmd, rest)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m rest \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msendcmd(\u001b[39m\"\u001b[39m\u001b[39mREST \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m rest)\n\u001b[0;32m--> 359\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msendcmd(cmd)\n\u001b[1;32m    360\u001b[0m \u001b[39m# Some servers apparently send a 200 reply to\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m# a LIST or STOR command, before the 150 reply\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m# (and way before the 226 reply). This seems to\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# be in violation of the protocol (which only allows\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m# 1xx or error messages for LIST), so we just discard\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m# this response.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m resp[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/ftplib.py:281\u001b[0m, in \u001b[0;36mFTP.sendcmd\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Send a command and return the response.'''\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputcmd(cmd)\n\u001b[0;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetresp()\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/ftplib.py:244\u001b[0m, in \u001b[0;36mFTP.getresp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetresp\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 244\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetmultiline()\n\u001b[1;32m    245\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugging:\n\u001b[1;32m    246\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m*resp*\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msanitize(resp))\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/ftplib.py:230\u001b[0m, in \u001b[0;36mFTP.getmultiline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetmultiline\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 230\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetline()\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m line[\u001b[39m3\u001b[39m:\u001b[39m4\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    232\u001b[0m         code \u001b[39m=\u001b[39m line[:\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/ftplib.py:212\u001b[0m, in \u001b[0;36mFTP.getline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetline\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 212\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile\u001b[39m.\u001b[39mreadline(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxline \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxline:\n\u001b[1;32m    214\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mgot more than \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxline)\n",
      "File \u001b[0;32m~/mambaforge/envs/snow-hydrology/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a list of datetime objects for every day between the provided start_date and end_date. \n",
    "datelist = pd.date_range(\n",
    "    dt.datetime.strptime(start_date, '%Y%m%d'),\n",
    "    dt.datetime.strptime(end_date, '%Y%m%d'),\n",
    "    freq='d'\n",
    ").strftime('%Y%m%d').tolist()\n",
    "\n",
    "# Download one daily file for each date in the datelist. \n",
    "# cache = True so that when we rerun the notebook in the future, we don't have to redownload the data.\n",
    "files = [download_sos_data_day(date, download_directory, cache=True) for date in datelist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and and concatenate SoS datasets with Xarray\n",
    "\n",
    "Note that as we open each individual file, we extract only the VARIABLES we defined above.  This may also take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up one day's dataset. We open this single dataset and keep all variables, so we can check out all that is available\n",
    "example_complete_dataset = xr.open_dataset(files[0])\n",
    "\n",
    "# Open up datasets from every day, extracting our VARIABLES of choice as we open individual datasets. This is so that we have a smaller dataset held \n",
    "# in computer memory.\n",
    "all_datasets = [xr.open_dataset(file)[VARIABLES] for file in files]\n",
    "sos_dataset = xr.concat(all_datasets, dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out our SoS dataset\n",
    "\n",
    "Note that our `dataset` has just the variables we are defined. Note that `example_complete_dataset` has 1047 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_complete_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open precipitation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_dataset = xr.open_dataset(precipitation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out the precipitation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "We use the Matplotlib library because integrates nicely with Xarray and makes creating simple plots quickly an easy task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting, we resample the SoS dataset by finding the hourly mean. We don't need the high-rate (5 minute) data to look at season long changes. Also, the snow pillow dataset has lots of nans so resampling makes the plot look better. The precipitaton dataset is already daily, so we don't need to resample it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_dataset_60min = sos_dataset.resample(time=\"60Min\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data. Note that SWE_p1_c, SWE_p2_c, SWE_p3_c, SWE_p24_ corresponse to the snow pillows beneath towers UW, UE, C, D, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(12,9), sharex=True)\n",
    "sos_dataset_60min['SWE_p1_c'].plot(ax=axes[0], label='SWE_p1_c')\n",
    "sos_dataset_60min['SWE_p2_c'].plot(ax=axes[0], label='SWE_p2_c')\n",
    "sos_dataset_60min['SWE_p3_c'].plot(ax=axes[0], label='SWE_p3_c')\n",
    "sos_dataset_60min['SWE_p4_c'].plot(ax=axes[0], label='SWE_p4_c')\n",
    "sos_dataset_60min['T_2m_c'].plot(ax=axes[1])\n",
    "precip_dataset['acc_prec'].plot(ax=axes[2])\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Snow pillow,\\nSnow Water Equivalent [mm]\")\n",
    "axes[0].set_xlim(dt.datetime(2022,12,1), dt.datetime(2023,3,27))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now print out the 60 minute date to a netcdf file for ease of use in the future\n",
    "sos_dataset_60min.to_file(\"sos_SWE_T_P.cdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_dataset_60min.to_netcdf(\"sos_SWE_T_P.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdataset=xr.open_dataset(\"sos_SWE_T_P.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
